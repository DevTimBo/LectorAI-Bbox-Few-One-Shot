{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors: Jason Pranata and Tim Harmling\n",
    "Based on Nathans Old Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn_v2\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import torchvision.ops as ops\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "import os\n",
    "from tqdm  import tqdm\n",
    "from pycocotools.coco import COCO\n",
    "import glob\n",
    "import json\n",
    "from cjm_torchvision_tfms.core import ResizeMax, PadSquare, CustomRandomIoUCrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('CUDA is available!')\n",
    "else:\n",
    "    print('CUDA is not available.')\n",
    "\n",
    "# Print the CUDA device count\n",
    "print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "\n",
    "# Get the current CUDA device\n",
    "current_device = torch.cuda.current_device()\n",
    "print(f\"Current CUDA device: {current_device}\")\n",
    "\n",
    "# Print the name of the current CUDA device\n",
    "print(f\"Current CUDA device name: {torch.cuda.get_device_name(current_device)}\")\n",
    "torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = 'Bilder'\n",
    "ann_folder = 'all_json'\n",
    "EPOCHS = 300\n",
    "ES_START_EPOCH = 0\n",
    "PATIENCE = 500\n",
    "LR = 0.0005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import tv_tensors \n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root, ann_folder, class_list, transforms=None, update_paths=True):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.class_list = class_list\n",
    "        self.update_paths = update_paths\n",
    "\n",
    "        if update_paths:\n",
    "            self.update_json_image_paths(ann_folder)\n",
    "\n",
    "        self.dataset = []\n",
    "        ann_files = glob.glob(os.path.join(ann_folder, \"*.json\"))\n",
    "        for annFile in ann_files:\n",
    "            with open(annFile, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                self.dataset.append(data)\n",
    "            print(f\"Loaded {annFile} with {len(data['shapes'])} annotations\")\n",
    "\n",
    "        self.img_ids = [i for i in range(len(self.dataset))]\n",
    "        print(f\"Total unique image IDs: {len(self.img_ids)}\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.img_ids):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "\n",
    "\n",
    "        img_id = self.img_ids[idx]\n",
    "        original_width, original_height = self._get_original_image_size_from_json(img_id)\n",
    "        sample = self._load_image_and_annotations(img_id, original_width, original_height)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            sample = self.transforms(sample)\n",
    "\n",
    "        img = sample[\"image\"]\n",
    "        target = {\n",
    "            \"boxes\": sample[\"box\"],\n",
    "            \"labels\": sample[\"label\"],\n",
    "            \"masks\": sample[\"mask\"],\n",
    "        }\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def _load_image_and_annotations(self, img_id, original_width, original_height):\n",
    "        data = self.dataset[img_id]\n",
    "\n",
    "        img_path = os.path.join(self.root, os.path.basename(data['imagePath']))\n",
    "        if not os.path.exists(img_path):\n",
    "            raise FileNotFoundError(f\"Image file {img_path} not found\")\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        boxes, labels = self._get_annotation_data_from_json(data['shapes'], original_width, original_height)\n",
    "        masks = np.array(self.create_segmentation_masks((1024, 1024), boxes))\n",
    "\n",
    "        sample = {\n",
    "            \"image\": tv_tensors.Image(img),\n",
    "            \"box\": tv_tensors.BoundingBoxes(torch.as_tensor(boxes, dtype=torch.float32), format=\"xyxy\", canvas_size=(1024, 1024)),\n",
    "            \"label\": torch.as_tensor(labels, dtype=torch.int64),\n",
    "            \"mask\": tv_tensors.Mask(torch.tensor(masks, dtype=torch.uint8)),\n",
    "        }\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    def _get_original_image_size_from_json(self, img_id):\n",
    "        data = self.dataset[img_id]\n",
    "        return data['imageWidth'], data['imageHeight']\n",
    "\n",
    "    def _get_annotation_data_from_json(self, shapes, original_width, original_height, target_size=1024):\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        for ann in shapes:\n",
    "            points = ann['points']\n",
    "            xmin = min(point[0] for point in points)\n",
    "            ymin = min(point[1] for point in points)\n",
    "            xmax = max(point[0] for point in points)\n",
    "            ymax = max(point[1] for point in points)\n",
    "            \n",
    "            # Normalize bounding box coordinates based on original image size\n",
    "            xmin_norm = xmin * target_size / original_width\n",
    "            ymin_norm = ymin * target_size / original_height\n",
    "            xmax_norm = xmax * target_size / original_width\n",
    "            ymax_norm = ymax * target_size / original_height\n",
    "\n",
    "            boxes.append([xmin_norm, ymin_norm, xmax_norm, ymax_norm])\n",
    "            labels.append(self.class_list.index(ann['label']))\n",
    "\n",
    "        return boxes, labels\n",
    "\n",
    "    def create_segmentation_masks(self, image_size, boxes):\n",
    "        masks = []\n",
    "        for box in boxes:\n",
    "            mask = Image.new('L', image_size, 0)\n",
    "            draw = ImageDraw.Draw(mask)\n",
    "            xmin, ymin, xmax, ymax = box\n",
    "            polygon_coords = [(xmin, ymin), (xmax, ymin), (xmax, ymax), (xmin, ymax)]\n",
    "            draw.polygon(polygon_coords, fill=1)\n",
    "            mask_np = np.array(mask)\n",
    "            masks.append(mask_np)\n",
    "        return masks\n",
    "\n",
    "    def update_json_image_paths(self, ann_folder):\n",
    "        json_files = glob.glob(os.path.join(ann_folder, '*.json'))\n",
    "\n",
    "        def update_json_file(json_file):\n",
    "            with open(json_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            old_path = data['imagePath']\n",
    "            image_basename = os.path.basename(old_path.replace('\\\\', '/'))\n",
    "            new_path = os.path.join(self.root, image_basename)\n",
    "            data['imagePath'] = new_path\n",
    "\n",
    "            with open(json_file, 'w') as f:\n",
    "                json.dump(data, f, indent=4)\n",
    "\n",
    "        for json_file in json_files:\n",
    "            update_json_file(json_file)\n",
    "            print(f\"Updated image paths in {json_file}\")\n",
    "\n",
    "        print(\"All JSON files updated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = [\n",
    "    \"background\",\n",
    "    \"ad_unterschrift\", \"adress_aend\", \"ad_erzieher\", \"ad_neue_ad\", \"ad_schueler_unterschrift\",\n",
    "    \"ad_erzieher_name\", \"ad_erzieher_vorname\", \"ad_erzieher_tel\", \"ad_erzieher_email\",\n",
    "    \"ad_neue_ad_str_haus_nr\", \"ad_neue_ad_plz\", \"ad_neue_ad_stadt\", \"ad_schueler_datum\",\n",
    "    \"schueler\", \"schueler_name\", \"schueler_vorname\", \"schueler_klasse\",\n",
    "    \"ag\", \"ag_auswahl\", \"ag_unterschrift\", \"ag_schueler_datum\",\n",
    "    \"ag_auswahl_wahl_1\", \"ag_auswahl_wahl_2\", \"ag_auswahl_wahl_3\", \"ag_schueler_unterschrift\"]#,\n",
    "    #\"AL\", \"al_1\", \"al_2\", \"al_3\", \"allergien\", \"Sonstiges\" ]\n",
    "\n",
    "print(len(class_list))\n",
    "\n",
    "iou_crop = CustomRandomIoUCrop(min_scale=0.3,\n",
    "                            max_scale=1.0,\n",
    "                            min_aspect_ratio=0.5,\n",
    "                            max_aspect_ratio=2.0,\n",
    "                            sampler_options=[0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0],\n",
    "                            trials=400,\n",
    "                            jitter_factor=0.25)\n",
    "\n",
    "# Compose transforms for data augmentation\n",
    "data_aug_tfms = transforms.Compose(\n",
    "    transforms=[\n",
    "        transforms.ToImage(), \n",
    "        transforms.ToDtype(torch.float32, scale=True),\n",
    "        iou_crop,\n",
    "        transforms.ColorJitter(\n",
    "                brightness = (0.875, 1.125),\n",
    "                contrast = (0.5, 1.5),\n",
    "                saturation = (0.5, 1.5),\n",
    "                hue = (-0.05, 0.05),\n",
    "        ),\n",
    "        transforms.RandomGrayscale(),\n",
    "        transforms.RandomEqualize(),\n",
    "        transforms.RandomPosterize(bits=3, p=0.5),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "    ],\n",
    ")\n",
    "# Create a `ResizeMax` object\n",
    "resize_max = ResizeMax(max_sz=1024)\n",
    "\n",
    "# Create a `PadSquare` object\n",
    "pad_square = PadSquare(shift=True, fill=0)\n",
    "\n",
    "# Compose transforms to resize and pad input images\n",
    "resize_pad_tfm = transforms.Compose([\n",
    "    resize_max,\n",
    "    pad_square,\n",
    "    transforms.Resize([1024] * 2, antialias=True)\n",
    "])\n",
    "\n",
    "# Compose transforms to sanitize bounding boxes and normalize input data\n",
    "final_tfms = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.SanitizeBoundingBoxes(),\n",
    "])\n",
    "\n",
    "# Define the transformations for training and validation datasets\n",
    "train_tfms = transforms.Compose([\n",
    "    data_aug_tfms,\n",
    "    resize_pad_tfm,\n",
    "    final_tfms\n",
    "])\n",
    "valid_tfms = transforms.Compose([resize_pad_tfm, final_tfms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset = CustomDataset(\n",
    "    root_folder, ann_folder, class_list, None\n",
    ")\n",
    "\n",
    "\n",
    "# Check if the dataset is loaded correctly\n",
    "print(f\"Total number of samples: {len(complete_dataset)}\")\n",
    "for i in range(1):\n",
    "    try:\n",
    "        img, target = complete_dataset[i]\n",
    "        print(f\"Sample {i + 1}:\")\n",
    "        print(f\" - Image shape: {img.shape}\")\n",
    "        print(f\" - Number of boxes: {len(target['boxes'])}\")\n",
    "        print(f\" - Boxes: {target['boxes']}\")\n",
    "        print(f\" - Labels: {target['labels']}\")\n",
    "        print(f\" - Masks shape: {target['masks'].shape}\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sample {i + 1}: {e}\")\n",
    "\n",
    "print(f\"Total number of samples: {len(complete_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainDataset(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, target = self.subset[idx]\n",
    "\n",
    "        sample = {\n",
    "            \"image\": img,\n",
    "            \"boxes\": target[\"boxes\"],\n",
    "            \"labels\": target[\"labels\"],\n",
    "            \"masks\": target[\"masks\"],\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        img = sample[\"image\"]\n",
    "        target = {\n",
    "            \"boxes\": sample[\"boxes\"],\n",
    "            \"labels\": sample[\"labels\"],\n",
    "            \"masks\": sample[\"masks\"],\n",
    "        }\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset lengths\n",
    "train_size = int(0.75 * len(complete_dataset))\n",
    "val_size = int(0.2 * len(complete_dataset))\n",
    "test_size = len(complete_dataset) - train_size - val_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset, test_dataset = random_split(complete_dataset, [train_size, val_size, test_size])\n",
    "train_dataset = CustomTrainDataset(train_dataset, transform=train_tfms)\n",
    "\n",
    "# Define data loaders\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    "    collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "val_data_loader = DataLoader(\n",
    "    val_dataset, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "    test_dataset, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image, boxes, labels, masks, class_list=None):\n",
    "    fig, ax = plt.subplots(1)\n",
    "    image = image.permute(1, 2, 0).cpu().numpy()\n",
    "    ax.imshow(image)\n",
    "\n",
    "    for mask in masks:\n",
    "        if mask.ndim == 2:\n",
    "            mask = np.expand_dims(mask, axis=0)\n",
    "        masked_image = np.ma.masked_where(mask[0] == 0, mask[0])\n",
    "        ax.imshow(masked_image, alpha=0.5, cmap='jet')\n",
    "\n",
    "    for box, label in zip(boxes, labels):\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        width = xmax - xmin\n",
    "        height = ymax - ymin\n",
    "        rect = patches.Rectangle((xmin, ymin), width, height, linewidth=0.2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(xmin, ymin, class_list[label], fontsize=6, color='r')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loader(data_loader, name, class_list):\n",
    "    print(f\"Testing {name} data loader...\")\n",
    "    for i, (images, targets) in enumerate(data_loader):\n",
    "        print(f\"Batch {i + 1}:\")\n",
    "        print(f\" - Number of images: {len(images)}\")\n",
    "        print(f\" - Image size: {images[0].shape}\")\n",
    "        print(f\" - Number of targets: {len(targets)}\")\n",
    "        for j in range(len(targets)):\n",
    "            boxes = targets[j]['boxes']\n",
    "            labels = targets[j]['labels']\n",
    "            masks = targets[j]['masks']\n",
    "            \n",
    "            print(f\" - Target {j + 1}:\")\n",
    "            print(f\"   - Boxes shape: {boxes.shape}\")\n",
    "            print(f\"   - Labels shape: {labels.shape}\")\n",
    "            print(f\" - Mask shape: {targets[j]['masks'].shape}\")\n",
    "            \n",
    "            plot_image(images[j], boxes, labels, masks, class_list)\n",
    "        break\n",
    "\n",
    "# Test each data loader\n",
    "for i in range(10):\n",
    "    test_loader(train_data_loader, \"train\", class_list)\n",
    "\n",
    "# test_loader(val_data_loader, \"validation\", class_list)\n",
    "# test_loader(test_data_loader, \"test\", class_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=50, delta=0, verbose=False, start_epoch=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.verbose = verbose\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.start_epoch = start_epoch\n",
    "\n",
    "    def __call__(self, val_loss, model, epoch):\n",
    "        if epoch < self.start_epoch:\n",
    "            return\n",
    "\n",
    "        score = -val_loss\n",
    "        self.save_every_nth_epochs(model, epoch)\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_every_nth_epochs(self, model, epoch):\n",
    "        if epoch % 25 == 0:\n",
    "            torch.save(model.state_dict(),f'{epoch}_checkpoint.pth')\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decreases.'''\n",
    "        if self.verbose:\n",
    "            print(f\"Validation loss decreased ({self.best_loss:.6f} --> {val_loss:.6f}).  Saving model ...\")\n",
    "        torch.save(model.state_dict(), 'checkpoint.pth')\n",
    "        self.best_loss = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred_boxes, pred_labels, pred_scores, pred_masks, true_boxes, true_labels, true_masks, iou_threshold):\n",
    "    num_classes = np.max(np.concatenate([pred_labels, true_labels])) + 1\n",
    "    tp = np.zeros(num_classes)\n",
    "    fp = np.zeros(num_classes)\n",
    "    fn = np.zeros(num_classes)\n",
    "    matched = [[] for _ in range(num_classes)]\n",
    "\n",
    "    for pred_box, pred_label, pred_mask in zip(pred_boxes, pred_labels, pred_masks):\n",
    "        match_found = False\n",
    "        for idx, (true_box, true_label, true_mask) in enumerate(zip(true_boxes, true_labels, true_masks)):\n",
    "            if pred_label == true_label:\n",
    "                box_iou = compute_box_iou(pred_box, true_box)\n",
    "                mask_iou = compute_mask_iou(pred_mask, true_mask)\n",
    "                \n",
    "                if box_iou >= iou_threshold and mask_iou >= iou_threshold and idx not in matched[true_label]:\n",
    "                    tp[true_label] += 1\n",
    "                    matched[true_label].append(idx)\n",
    "                    match_found = True\n",
    "                    break\n",
    "        if not match_found:\n",
    "            fp[pred_label] += 1\n",
    "\n",
    "    for idx, true_label in enumerate(true_labels):\n",
    "        if idx not in matched[true_label]:\n",
    "            fn[true_label] += 1\n",
    "\n",
    "    precision = np.divide(tp, tp + fp, out=np.zeros_like(tp), where=(tp + fp) > 0)\n",
    "    recall = np.divide(tp, tp + fn, out=np.zeros_like(tp), where=(tp + fn) > 0)\n",
    "    accuracy = np.divide(tp, tp + fp + fn, out=np.zeros_like(tp), where=(tp + fp + fn) > 0)\n",
    "    ap = compute_ap(tp, fp, fn)\n",
    "    mAP = np.mean(ap)\n",
    "\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'accuracy': accuracy,\n",
    "        'mAP': mAP\n",
    "    }\n",
    "\n",
    "def compute_box_iou(box1, box2):\n",
    "    x1_max = max(box1[0], box2[0])\n",
    "    y1_max = max(box1[1], box2[1])\n",
    "    x2_min = min(box1[2], box2[2])\n",
    "    y2_min = min(box1[3], box2[3])\n",
    "\n",
    "    intersection_area = max(0, x2_min - x1_max) * max(0, y2_min - y1_max)\n",
    "\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "\n",
    "    iou = intersection_area / float(box1_area + box2_area - intersection_area)\n",
    "    return iou\n",
    "\n",
    "def compute_mask_iou(mask1, mask2):\n",
    "    intersection = np.logical_and(mask1, mask2).sum()\n",
    "    union = np.logical_or(mask1, mask2).sum()\n",
    "    iou = intersection / union\n",
    "    return iou\n",
    "\n",
    "def compute_ap(tp, fp, fn):\n",
    "    precision = np.divide(tp, tp + fp, out=np.zeros_like(tp), where=(tp + fp) > 0)\n",
    "    recall = np.divide(tp, tp + fn, out=np.zeros_like(tp), where=(tp + fn) > 0)\n",
    "    ap = precision * recall\n",
    "    return ap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingHistory:\n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "        self.precisions = []\n",
    "        self.recalls = []\n",
    "        self.accuracies = []\n",
    "        self.mAPs = []\n",
    "        self.lr = []\n",
    "\n",
    "    def log_metrics(self, metrics_dict):\n",
    "        self.losses.append(metrics_dict['val_loss'])\n",
    "        self.precisions.append(metrics_dict['precision'])\n",
    "        self.recalls.append(metrics_dict['recall'])\n",
    "        self.accuracies.append(metrics_dict['accuracy'])\n",
    "        self.mAPs.append(metrics_dict['mAP'])\n",
    "        self.lr.append(metrics_dict['lr'])\n",
    "    \n",
    "    def get_history(self):\n",
    "        return {\n",
    "            'val_loss': self.losses,\n",
    "            'precisions': self.precisions,\n",
    "            'recalls': self.recalls,\n",
    "            'accuracies': self.accuracies,\n",
    "            'mAPs': self.mAPs,\n",
    "            'lr': self.lr\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch(model, optimizer, data_loader, device, progress_bar, scheduler=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    bar = tqdm(data_loader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for images, targets in bar:\n",
    "        images = [image.to(device).float() / 255.0 for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        running_loss += losses.item()\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    return running_loss / len(data_loader)\n",
    "\n",
    "def evaluate(model, data_loader, device, progress_bar, iou_threshold=0.9, plot = False):\n",
    "    model.eval()\n",
    "    metrics = {\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'accuracy': [],\n",
    "        'mAP': []\n",
    "    }\n",
    "    running_val_loss = 0.0\n",
    "\n",
    "    bar = tqdm(data_loader, desc=\"Validating\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for images, targets in bar:\n",
    "            images = [image.to(device).float() / 255.0 for image in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            outputs = model(images)\n",
    "            \n",
    "            for i, output in enumerate(outputs):\n",
    "                keep = []\n",
    "                unique_labels = output['labels'].unique()\n",
    "                for label in unique_labels:\n",
    "                    class_mask = output['labels'] == label\n",
    "                    boxes = output['boxes'][class_mask]\n",
    "                    scores = output['scores'][class_mask]\n",
    "                    class_keep = ops.nms(boxes, scores, iou_threshold)\n",
    "                    keep.extend(class_keep.cpu().numpy())\n",
    "                keep = torch.as_tensor(keep, dtype=torch.long, device=device)\n",
    "                outputs[i] = {k: v[keep] for k, v in output.items()}\n",
    "            model.train()\n",
    "            loss_dict = model(images, targets)\n",
    "            model.eval()\n",
    "\n",
    "            if isinstance(loss_dict, dict):\n",
    "                val_loss = sum(loss for loss in loss_dict.values()).item()\n",
    "            else:\n",
    "                val_loss = sum(loss for loss in loss_dict).item()\n",
    "            \n",
    "            running_val_loss += val_loss\n",
    "            progress_bar.update(1)\n",
    "\n",
    "            for output, target in zip(outputs, targets):\n",
    "                pred_boxes = output['boxes'].cpu().numpy()\n",
    "                pred_labels = output['labels'].cpu().numpy()\n",
    "                pred_scores = output['scores'].cpu().numpy()\n",
    "                pred_masks = output['masks'].cpu().numpy() > 0.5\n",
    "                true_boxes = target['boxes'].cpu().numpy()\n",
    "                true_labels = target['labels'].cpu().numpy()\n",
    "                true_masks = target['masks'].cpu().numpy() > 0.5\n",
    "                \n",
    "                metric = compute_metrics(pred_boxes, pred_labels, pred_scores, pred_masks,\n",
    "                                        true_boxes, true_labels, true_masks, iou_threshold)\n",
    "                metrics['precision'].append(metric['precision'])\n",
    "                metrics['recall'].append(metric['recall'])\n",
    "                metrics['accuracy'].append(metric['accuracy'])\n",
    "                metrics['mAP'].append(metric['mAP'])\n",
    "                \n",
    "                if plot:\n",
    "                    print(\"Predicted boxes, labels, and masks:\")\n",
    "                    plot_image(images[0], pred_boxes, pred_labels, pred_masks)\n",
    "                    print(\"Ground truth boxes, labels, and masks:\")\n",
    "                    plot_image(images[0], true_boxes, true_labels, true_masks)\n",
    "\n",
    "    max_classes = max(len(p) for p in metrics['precision'])\n",
    "    precision_padded = [np.pad(p, (0, max_classes - len(p)), 'constant') for p in metrics['precision']]\n",
    "    recall_padded = [np.pad(r, (0, max_classes - len(r)), 'constant') for r in metrics['recall']]\n",
    "    accuracy_padded = [np.pad(a, (0, max_classes - len(a)), 'constant') for a in metrics['accuracy']]\n",
    "\n",
    "    avg_precision = np.mean(np.stack(precision_padded), axis=0)\n",
    "    avg_recall = np.mean(np.stack(recall_padded), axis=0)\n",
    "    avg_accuracy = np.mean(np.stack(accuracy_padded), axis=0)\n",
    "    avg_mAP = np.mean(metrics['mAP'])\n",
    "    avg_val_loss = running_val_loss / len(data_loader)\n",
    "\n",
    "    return avg_val_loss, avg_precision, avg_recall, avg_accuracy, avg_mAP\n",
    "\n",
    "def main_training_loop(model, optimizer, scheduler, train_loader, val_loader, history, device, num_epochs, patience=PATIENCE, start_epoch=ES_START_EPOCH):\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True, start_epoch=start_epoch)\n",
    "\n",
    "    # Calculate the total number of steps\n",
    "    total_steps = num_epochs * (len(train_loader) + len(val_loader))\n",
    "\n",
    "    with tqdm(total=total_steps, desc=\"Training Progress\") as pbar:\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}:\")\n",
    "\n",
    "            # Training phase\n",
    "            train_loss = one_epoch(model, optimizer, train_loader, device, pbar, scheduler)\n",
    "            print(\"Train Loss: {:.4f}\".format(train_loss))\n",
    "            \n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"Learning rate: {current_lr}\")\n",
    "\n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss, avg_precision, avg_recall, avg_accuracy, avg_mAP = evaluate(model, val_loader, device, pbar)\n",
    "            history.log_metrics({\"val_loss\": val_loss, \"precision\": avg_precision, \"recall\": avg_recall, \n",
    "                                \"accuracy\": avg_accuracy, \"mAP\": avg_mAP, \"lr\": current_lr})\n",
    "            print(\"Validation Loss: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, Accuracy: {:.4f}, mAP: {:.4f}\".format(\n",
    "                val_loss,\n",
    "                np.mean(avg_precision),\n",
    "                np.mean(avg_recall),\n",
    "                np.mean(avg_accuracy),\n",
    "                avg_mAP\n",
    "            ))\n",
    "            # Check early stopping\n",
    "            early_stopping(val_loss, model, epoch)\n",
    "\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping: No improvement in the last {} epochs\".format(patience))\n",
    "                break\n",
    "    model.load_state_dict(torch.load('checkpoint.pth'))\n",
    "    torch.save(model.state_dict(), 'best_model.pth')\n",
    "    print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and adjust untrained Mask R-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFER_LEARNING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_list)\n",
    "device = torch.device(\"cuda\")\n",
    "if TRANSFER_LEARNING:\n",
    "    model_state_dict = torch.load('_tim_trained.pth')\n",
    "    model = maskrcnn_resnet50_fpn_v2()\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes = num_classes)\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    dim_reduced = model.roi_heads.mask_predictor.conv5_mask.out_channels\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, dim_reduced, num_classes)\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()  # \n",
    "else:\n",
    "    model = maskrcnn_resnet50_fpn_v2(weights='DEFAULT')\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes = num_classes)\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    dim_reduced = model.roi_heads.mask_predictor.conv5_mask.out_channels\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, dim_reduced, num_classes)\n",
    "\n",
    "model.to(device)\n",
    "history = TrainingHistory()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, \n",
    "                                                    max_lr=LR, \n",
    "                                                    total_steps=EPOCHS*len(train_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_training_loop(model, optimizer, lr_scheduler, train_data_loader, val_data_loader, history, device, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history_dict):\n",
    "    epochs = range(1, len(history_dict['val_loss']) + 1)\n",
    "    best_val_loss = min(history_dict['val_loss'])\n",
    "    best_val_loss_idx = history_dict['val_loss'].index(best_val_loss)\n",
    "    best_map = max(history_dict['mAPs'])\n",
    "    best_map_idx = history_dict['mAPs'].index(best_map)\n",
    "    learning_rates = history_dict.get('lr', [])\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(epochs, history_dict['val_loss'], label='Validation Loss')\n",
    "    plt.title('Validation Loss: {:.4f} (Epoch: {})'.format(best_val_loss, best_val_loss_idx))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(epochs, history_dict['mAPs'], label='mAP')\n",
    "    plt.title('Mean Average Precision (mAP): {:.4f} (Epoch: {})'.format(best_map, best_map_idx))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('mAP')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(epochs, learning_rates, label='Learning Rate')\n",
    "    plt.title('Learning Rate over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "history_dict = history.get_history()\n",
    "plot_training_history(history_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict = torch.load('checkpoint.pth')\n",
    "model = maskrcnn_resnet50_fpn_v2()\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes = num_classes)\n",
    "in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "dim_reduced = model.roi_heads.mask_predictor.conv5_mask.out_channels\n",
    "model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, dim_reduced, num_classes)\n",
    "model.load_state_dict(model_state_dict)\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, targets in test_data_loader:\n",
    "    images = list(image.to(device).float() / 255.0 for image in images)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "    for output, target in zip(outputs, targets):\n",
    "        pred_boxes = output['boxes'].detach().cpu().numpy()\n",
    "        pred_labels = output['labels'].detach().cpu().numpy()\n",
    "        pred_scores = output['scores'].detach().cpu().numpy()\n",
    "        pred_masks = output['masks'].detach().cpu().numpy() > 0.5\n",
    "        true_boxes = target['boxes'].detach().cpu().numpy()\n",
    "        true_labels = target['labels'].detach().cpu().numpy()\n",
    "        true_masks = target['masks'].detach().cpu().numpy() > 0.5\n",
    "        print(\"Predicted boxes, labels, and masks:\")\n",
    "        plot_image(images[0], pred_boxes, pred_labels, pred_masks, class_list)\n",
    "        print(\"Ground truth boxes, labels, and masks:\")\n",
    "        plot_image(images[0], true_boxes, true_labels, true_masks, class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "image = cv2.imread(\"/mnt/c/Users/jason/GitHubRepos/LectorAI-TextExtraction/tempimages_api/beispiel_form_covered.jpg\")\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image = T.ToTensor()(image)\n",
    "image = T.Resize((1024, 1024))(image)\n",
    "image = image.unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(image)\n",
    "pred_boxes = output['boxes'].detach().cpu().numpy()\n",
    "pred_labels = output['labels'].detach().cpu().numpy()\n",
    "pred_scores = output['scores'].detach().cpu().numpy()\n",
    "pred_masks = output['masks'].detach().cpu().numpy() > 0.9\n",
    "image = image.squeeze(0).cpu()\n",
    "plot_image(image, pred_boxes, pred_labels, pred_masks, class_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "everything",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
