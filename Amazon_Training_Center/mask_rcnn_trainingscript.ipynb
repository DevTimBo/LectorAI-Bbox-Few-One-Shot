{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_Weights\n",
    "import torchvision.ops as ops\n",
    "import torch.utils.data\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.transforms import transforms as T\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('CUDA is available!')\n",
    "else:\n",
    "    print('CUDA is not available.')\n",
    "\n",
    "# Print the CUDA device count\n",
    "print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "\n",
    "# Get the current CUDA device\n",
    "current_device = torch.cuda.current_device()\n",
    "print(f\"Current CUDA device: {current_device}\")\n",
    "\n",
    "# Print the name of the current CUDA device\n",
    "print(f\"Current CUDA device name: {torch.cuda.get_device_name(current_device)}\")\n",
    "torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 300\n",
    "SCHEDULER_STEP_SIZE = 40\n",
    "ES_START_EPOCH = SCHEDULER_STEP_SIZE + 10\n",
    "PATIENCE = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PascalVOCDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, class_list, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"Images\"))))\n",
    "        self.anns = list(sorted(os.listdir(os.path.join(root, \"XML\"))))\n",
    "        self.class_list = class_list\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"Images\", self.imgs[idx])\n",
    "        ann_path = os.path.join(self.root, \"XML\", self.anns[idx])\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = T.ToTensor()(img) \n",
    "        original_width, original_height = self._get_original_image_size_from_xml(ann_path)\n",
    "        \n",
    "        boxes, labels = self._get_annotation_data_from_xml(ann_path, original_width, original_height)\n",
    "        masks = np.array(self.create_segmentation_masks((1024,1024), boxes))\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": torch.as_tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.as_tensor(labels, dtype=torch.int64),\n",
    "            \"masks\": torch.tensor(masks, dtype=torch.uint8),\n",
    "        }\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    # Some Images in the dataset has already been resized to 1024 but the annotated bounding boxes are not normalized\n",
    "    def _get_original_image_size_from_xml(self, xml_path):\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        size = root.find(\"size\")\n",
    "        width = int(size.find(\"width\").text)\n",
    "        height = int(size.find(\"height\").text)\n",
    "        return width, height\n",
    "\n",
    "    def _get_annotation_data_from_xml(self, xml_path, original_width, original_height, target_size=1024):\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        for obj in root.findall(\"object\"):\n",
    "            bbox = obj.find(\"bndbox\")\n",
    "            xmin = float(bbox.find(\"xmin\").text)\n",
    "            ymin = float(bbox.find(\"ymin\").text)\n",
    "            xmax = float(bbox.find(\"xmax\").text)\n",
    "            ymax = float(bbox.find(\"ymax\").text)\n",
    "\n",
    "            # Normalize bounding box coordinates based on original image size\n",
    "            xmin_norm = xmin * target_size / original_width\n",
    "            ymin_norm = ymin * target_size / original_height\n",
    "            xmax_norm = xmax * target_size / original_width\n",
    "            ymax_norm = ymax * target_size / original_height\n",
    "\n",
    "            boxes.append([xmin_norm, ymin_norm, xmax_norm, ymax_norm])\n",
    "            \n",
    "            label = obj.find(\"name\").text\n",
    "            labels.append(self.class_list.index(label))\n",
    "        \n",
    "        return boxes, labels\n",
    "    \n",
    "    def create_segmentation_masks(self, image_size, boxes):\n",
    "        masks = []\n",
    "        for box in boxes:\n",
    "            mask = Image.new('L', image_size, 0)  # 'L' mode for grayscale\n",
    "            draw = ImageDraw.Draw(mask)\n",
    "            xmin, ymin, xmax, ymax = box\n",
    "            draw.rectangle([xmin, ymin, xmax, ymax], fill=1)\n",
    "            mask = np.array(mask)\n",
    "            masks.append(mask)\n",
    "        return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = [\n",
    "    \"ad_unterschrift\", \"adress_aend\", \"ad_erzieher\", \"ad_neue_ad\", \"ad_schueler_unterschrift\",\n",
    "    \"ad_erzieher_name\", \"ad_erzieher_vorname\", \"ad_erzieher_tel\", \"ad_erzieher_email\",\n",
    "    \"ad_neue_ad_str_haus_nr\", \"ad_neue_ad_plz\", \"ad_neue_ad_stadt\", \"ad_schueler_datum\",\n",
    "    \"schueler\", \"schueler_name\", \"schueler_vorname\", \"schueler_klasse\",\n",
    "    \"ag\", \"ag_auswahl\", \"ag_unterschrift\", \"ag_schueler_datum\",\n",
    "    \"ag_auswahl_wahl_1\", \"ag_auswahl_wahl_2\", \"ag_auswahl_wahl_3\", \"ag_schueler_unterschrift\"]\n",
    "\n",
    "print(len(class_list))\n",
    "\n",
    "transforms = T.Compose([T.Resize((1024, 1024))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset instances\n",
    "train_dataset = PascalVOCDataset('train_dataset', class_list= class_list, transforms=transforms )\n",
    "val_dataset = PascalVOCDataset('val_dataset', class_list= class_list, transforms=transforms )\n",
    "test_dataset = PascalVOCDataset('test_dataset', class_list= class_list, transforms=transforms)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Define data loaders\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=4, shuffle=True, num_workers=4,\n",
    "    collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "val_data_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=2, shuffle=False, num_workers=4,\n",
    "    collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image, boxes, labels, masks, class_list=None):\n",
    "    fig, ax = plt.subplots(1)\n",
    "    image = image.permute(1, 2, 0).cpu().numpy()\n",
    "    ax.imshow(image)\n",
    "\n",
    "    for mask in masks:\n",
    "        if mask.ndim == 2:\n",
    "            mask = np.expand_dims(mask, axis=0)\n",
    "        masked_image = np.ma.masked_where(mask[0] == 0, mask[0])\n",
    "        ax.imshow(masked_image, alpha=0.5, cmap='jet')\n",
    "\n",
    "    for box, label in zip(boxes, labels):\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        width = xmax - xmin\n",
    "        height = ymax - ymin\n",
    "        rect = patches.Rectangle((xmin, ymin), width, height, linewidth=0.2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(xmin, ymin, class_list[label], fontsize=6, color='r')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loader(data_loader, name, class_list):\n",
    "    print(f\"Testing {name} data loader...\")\n",
    "    for i, (images, targets) in enumerate(data_loader):\n",
    "        print(f\"Batch {i + 1}:\")\n",
    "        print(f\" - Number of images: {len(images)}\")\n",
    "        print(f\" - Image size: {images[0].shape}\")\n",
    "        print(f\" - Number of targets: {len(targets)}\")\n",
    "        for j in range(len(targets)):\n",
    "            boxes = targets[j]['boxes']\n",
    "            labels = targets[j]['labels']\n",
    "            masks = targets[j]['masks']\n",
    "            \n",
    "            print(f\" - Target {j + 1}:\")\n",
    "            print(f\"   - Boxes shape: {boxes.shape}\")\n",
    "            print(f\"   - Labels shape: {labels.shape}\")\n",
    "            print(f\" - Mask shape: {targets[j]['masks'].shape}\")\n",
    "            \n",
    "            plot_image(images[j], boxes, labels, masks, class_list)\n",
    "        break\n",
    "\n",
    "# Test each data loader\n",
    "test_loader(train_data_loader, \"train\", class_list)\n",
    "test_loader(val_data_loader, \"validation\", class_list)\n",
    "test_loader(test_data_loader, \"test\", class_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=50, delta=0, verbose=False, start_epoch=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.verbose = verbose\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.start_epoch = start_epoch\n",
    "\n",
    "    def __call__(self, val_loss, model, epoch):\n",
    "        if epoch < self.start_epoch:\n",
    "            return\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decreases.'''\n",
    "        if self.verbose:\n",
    "            print(f\"Validation loss decreased ({self.best_loss:.6f} --> {val_loss:.6f}).  Saving model ...\")\n",
    "        torch.save(model.state_dict(), 'checkpoint.pth')\n",
    "        self.best_loss = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred_boxes, pred_labels, pred_scores, pred_masks, true_boxes, true_labels, true_masks, iou_threshold=0.5):\n",
    "    num_classes = np.max(np.concatenate([pred_labels, true_labels])) + 1\n",
    "    tp = np.zeros(num_classes)\n",
    "    fp = np.zeros(num_classes)\n",
    "    fn = np.zeros(num_classes)\n",
    "    matched = [[] for _ in range(num_classes)]\n",
    "\n",
    "    for pred_box, pred_label, pred_mask in zip(pred_boxes, pred_labels, pred_masks):\n",
    "        match_found = False\n",
    "        for idx, (true_box, true_label, true_mask) in enumerate(zip(true_boxes, true_labels, true_masks)):\n",
    "            if pred_label == true_label:\n",
    "                box_iou = compute_box_iou(pred_box, true_box)\n",
    "                mask_iou = compute_mask_iou(pred_mask, true_mask)\n",
    "                \n",
    "                if box_iou >= iou_threshold and mask_iou >= iou_threshold and idx not in matched[true_label]:\n",
    "                    tp[true_label] += 1\n",
    "                    matched[true_label].append(idx)\n",
    "                    match_found = True\n",
    "                    break\n",
    "        if not match_found:\n",
    "            fp[pred_label] += 1\n",
    "\n",
    "    for idx, true_label in enumerate(true_labels):\n",
    "        if idx not in matched[true_label]:\n",
    "            fn[true_label] += 1\n",
    "\n",
    "    precision = np.divide(tp, tp + fp, out=np.zeros_like(tp), where=(tp + fp) > 0)\n",
    "    recall = np.divide(tp, tp + fn, out=np.zeros_like(tp), where=(tp + fn) > 0)\n",
    "    accuracy = np.divide(tp, tp + fp + fn, out=np.zeros_like(tp), where=(tp + fp + fn) > 0)\n",
    "    ap = compute_ap(tp, fp, fn)\n",
    "    mAP = np.mean(ap)\n",
    "\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'accuracy': accuracy,\n",
    "        'mAP': mAP\n",
    "    }\n",
    "\n",
    "def compute_box_iou(box1, box2):\n",
    "    x1_max = max(box1[0], box2[0])\n",
    "    y1_max = max(box1[1], box2[1])\n",
    "    x2_min = min(box1[2], box2[2])\n",
    "    y2_min = min(box1[3], box2[3])\n",
    "\n",
    "    intersection_area = max(0, x2_min - x1_max) * max(0, y2_min - y1_max)\n",
    "\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "\n",
    "    iou = intersection_area / float(box1_area + box2_area - intersection_area)\n",
    "    return iou\n",
    "\n",
    "def compute_mask_iou(mask1, mask2):\n",
    "    intersection = np.logical_and(mask1, mask2).sum()\n",
    "    union = np.logical_or(mask1, mask2).sum()\n",
    "    iou = intersection / union\n",
    "    return iou\n",
    "\n",
    "def compute_ap(tp, fp, fn):\n",
    "    precision = np.divide(tp, tp + fp, out=np.zeros_like(tp), where=(tp + fp) > 0)\n",
    "    recall = np.divide(tp, tp + fn, out=np.zeros_like(tp), where=(tp + fn) > 0)\n",
    "    ap = precision * recall\n",
    "    return ap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch(model, optimizer, data_loader, device, progress_bar):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    bar = tqdm(data_loader, desc=\"Training\", leave=False)\n",
    "    for images, targets in bar:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += losses.item()\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    return running_loss / len(data_loader)\n",
    "\n",
    "def evaluate(model, data_loader, device, progress_bar, iou_threshold=0.5, nms_threshold=0.5, plot = False):\n",
    "    model.eval()\n",
    "    metrics = {\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'accuracy': [],\n",
    "        'mAP': []\n",
    "    }\n",
    "    running_val_loss = 0.0\n",
    "\n",
    "    bar = tqdm(data_loader, desc=\"Validating\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for images, targets in bar:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            outputs = model(images)\n",
    "            \n",
    "            for i, output in enumerate(outputs):\n",
    "                keep = []\n",
    "                unique_labels = output['labels'].unique()\n",
    "                for label in unique_labels:\n",
    "                    class_mask = output['labels'] == label\n",
    "                    boxes = output['boxes'][class_mask]\n",
    "                    scores = output['scores'][class_mask]\n",
    "                    class_keep = ops.nms(boxes, scores, nms_threshold)\n",
    "                    keep.extend(class_keep.cpu().numpy())\n",
    "                keep = torch.as_tensor(keep, dtype=torch.long, device=device)\n",
    "                outputs[i] = {k: v[keep] for k, v in output.items()}\n",
    "            model.train()\n",
    "            loss_dict = model(images, targets)\n",
    "            model.eval()\n",
    "\n",
    "            if isinstance(loss_dict, dict):\n",
    "                val_loss = sum(loss for loss in loss_dict.values()).item()\n",
    "            else:\n",
    "                val_loss = sum(loss for loss in loss_dict).item()\n",
    "            \n",
    "            running_val_loss += val_loss\n",
    "            progress_bar.update(1)\n",
    "\n",
    "            for output, target in zip(outputs, targets):\n",
    "                pred_boxes = output['boxes'].cpu().numpy()\n",
    "                pred_labels = output['labels'].cpu().numpy()\n",
    "                pred_scores = output['scores'].cpu().numpy()\n",
    "                pred_masks = output['masks'].cpu().numpy() > 0.5\n",
    "                true_boxes = target['boxes'].cpu().numpy()\n",
    "                true_labels = target['labels'].cpu().numpy()\n",
    "                true_masks = target['masks'].cpu().numpy() > 0.5\n",
    "                \n",
    "                metric = compute_metrics(pred_boxes, pred_labels, pred_scores, pred_masks,\n",
    "                                        true_boxes, true_labels, true_masks, iou_threshold)\n",
    "                metrics['precision'].append(metric['precision'])\n",
    "                metrics['recall'].append(metric['recall'])\n",
    "                metrics['accuracy'].append(metric['accuracy'])\n",
    "                metrics['mAP'].append(metric['mAP'])\n",
    "                \n",
    "                if plot:\n",
    "                    print(\"Predicted boxes, labels, and masks:\")\n",
    "                    plot_image(images[0], pred_boxes, pred_labels, pred_masks)\n",
    "                    print(\"Ground truth boxes, labels, and masks:\")\n",
    "                    plot_image(images[0], true_boxes, true_labels, true_masks)\n",
    "\n",
    "    max_classes = max(len(p) for p in metrics['precision'])\n",
    "    precision_padded = [np.pad(p, (0, max_classes - len(p)), 'constant') for p in metrics['precision']]\n",
    "    recall_padded = [np.pad(r, (0, max_classes - len(r)), 'constant') for r in metrics['recall']]\n",
    "    accuracy_padded = [np.pad(a, (0, max_classes - len(a)), 'constant') for a in metrics['accuracy']]\n",
    "\n",
    "    avg_precision = np.mean(np.stack(precision_padded), axis=0)\n",
    "    avg_recall = np.mean(np.stack(recall_padded), axis=0)\n",
    "    avg_accuracy = np.mean(np.stack(accuracy_padded), axis=0)\n",
    "    avg_mAP = np.mean(metrics['mAP'])\n",
    "    avg_val_loss = running_val_loss / len(data_loader)\n",
    "\n",
    "    return avg_val_loss, avg_precision, avg_recall, avg_accuracy, avg_mAP\n",
    "\n",
    "def main_training_loop(model, optimizer, scheduler, train_loader, val_loader, device, num_epochs, patience=PATIENCE, start_epoch=ES_START_EPOCH):\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True, start_epoch=start_epoch)\n",
    "\n",
    "    # Calculate the total number of steps\n",
    "    total_steps = num_epochs * (len(train_loader) + len(val_loader))\n",
    "\n",
    "    with tqdm(total=total_steps, desc=\"Training Progress\") as pbar:\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}:\")\n",
    "\n",
    "            # Training phase\n",
    "            train_loss = one_epoch(model, optimizer, train_loader, device, pbar)\n",
    "            print(\"Train Loss: {:.4f}\".format(train_loss))\n",
    "\n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss, avg_precision, avg_recall, avg_accuracy, avg_mAP = evaluate(model, val_loader, device, pbar)\n",
    "            print(\"Validation Loss: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, Accuracy: {:.4f}, mAP: {:.4f}\".format(\n",
    "                val_loss,\n",
    "                np.mean(avg_precision),\n",
    "                np.mean(avg_recall),\n",
    "                np.mean(avg_accuracy),\n",
    "                avg_mAP\n",
    "            ))\n",
    "            if scheduler is not None:\n",
    "                scheduler.step(val_loss)\n",
    "            # Check early stopping\n",
    "            early_stopping(val_loss, model, epoch)\n",
    "\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping: No improvement in the last {} epochs\".format(patience))\n",
    "                break\n",
    "    model.load_state_dict(torch.load('checkpoint.pth'))\n",
    "    torch.save(model.state_dict(), 'best_model.pth')\n",
    "    print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and adjust untrained Mask R-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = maskrcnn_resnet50_fpn(weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "num_classes = len(class_list) + 1 \n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes = num_classes)\n",
    "#in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "#hidden_layer = 128\n",
    "#model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=0.0005) #SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=SCHEDULER_STEP_SIZE, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_training_loop(model, optimizer, None, train_data_loader, val_data_loader, device, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict = torch.load('best_model.pth')\n",
    "model = maskrcnn_resnet50_fpn()\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes = num_classes)\n",
    "model.load_state_dict(model_state_dict)\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, targets in test_data_loader:\n",
    "    images = list(image.to(device) for image in images)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "    for output, target in zip(outputs, targets):\n",
    "                pred_boxes = output['boxes'].detach().cpu().numpy()\n",
    "                pred_labels = output['labels'].detach().cpu().numpy()\n",
    "                pred_scores = output['scores'].detach().cpu().numpy()\n",
    "                pred_masks = output['masks'].detach().cpu().numpy() > 0.5\n",
    "                true_boxes = target['boxes'].detach().cpu().numpy()\n",
    "                true_labels = target['labels'].detach().cpu().numpy()\n",
    "                true_masks = target['masks'].detach().cpu().numpy() > 0.5\n",
    "                print(\"Predicted boxes, labels, and masks:\")\n",
    "                plot_image(images[0], pred_boxes, pred_labels, pred_masks, class_list)\n",
    "                print(\"Ground truth boxes, labels, and masks:\")\n",
    "                plot_image(images[0], true_boxes, true_labels, true_masks, class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"/mnt/c/Users/jason/GitHubRepos/LectorAI-TextExtraction/tempimages_api/beispiel_form_covered.jpg\")\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image = T.ToTensor()(image)\n",
    "image = T.Resize((1024, 1024))(image)\n",
    "image = image.unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(image)\n",
    "pred_boxes = output['boxes'].detach().cpu().numpy()\n",
    "pred_labels = output['labels'].detach().cpu().numpy()\n",
    "pred_scores = output['scores'].detach().cpu().numpy()\n",
    "pred_masks = output['masks'].detach().cpu().numpy() > 0.5\n",
    "image = image.squeeze(0).cpu()\n",
    "plot_image(image, pred_boxes, pred_labels, pred_masks, class_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "everything",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
